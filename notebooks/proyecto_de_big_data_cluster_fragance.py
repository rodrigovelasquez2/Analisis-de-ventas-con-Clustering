# -*- coding: utf-8 -*-
"""PROYECTO DE BIG DATA-CLUSTER: FRAGANCE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PsbK97qgy_vKNgfGefvG_kn_TqM4yO9v

# Ejemplo de Clasificación de Clientes con K-means en PySpark (CLUSTERES)
Se esta trabajando con la data sales_data_sample.csv
"""

!pip install pyspark

# Paso 1: Importar librerías
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
from pyspark.sql import functions as F
import pandas as pd

# Paso 2: Configuración de Spark
spark = SparkSession.builder \
    .appName("Clustering de Clientes") \
    .getOrCreate()

# Paso 3: Cargar datos
file_path = "fragance-dataset.csv"
df = spark.read.csv(file_path, header=True, inferSchema=True, sep=';')
print(df.columns)
column_types = df.dtypes

# Mostrar los resultados
for column, dtype in column_types:
    print(f"Columna: {column}, Tipo de dato: {dtype}")


df.show()

# Paso 4: Preprocesamiento
# Seleccionar características para el clustering
# Interpretación: Se Identifica grupos de clientes con comportamientos de compra similares.

features = ["QUANTITYORDERED", "PRICEEACH", "SALES"]
vec_assembler = VectorAssembler(inputCols=features, outputCol="features")
df_vector = vec_assembler.transform(df)

# Normalizar las características
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures", withMean=True, withStd=True)
scaler_model = scaler.fit(df_vector)
df_scaled = scaler_model.transform(df_vector)

# Paso 5: Aplicar K-means
kmeans = KMeans(featuresCol="scaledFeatures", k=5)  # 'k' es el número de clusteres
model = kmeans.fit(df_scaled)

# Predicción de clústeres
predictions = model.transform(df_scaled)

# Paso 6: Análisis de clústeres (Comportamiento de Compra)
clustering_analysis = predictions.groupBy("prediction").agg(
    F.avg("SALES").alias("Promedio Ventas"),
    F.avg("QUANTITYORDERED").alias("Promedio Cantidad Pedida"),
    F.count("ORDERNUMBER").alias("Número de Órdenes")
)

# Ordenar los resultados por 'Promedio Ventas' de mayor a menor
clustering_analysis_sorted = clustering_analysis.orderBy("Promedio Ventas", ascending=False)

# Mostrar los resultados ordenados
clustering_analysis_sorted.show(truncate=False)

"""# LISTAR CLUSTER"""

# Paso 7: Listar todos los clústeres
clusters = predictions.select("prediction").distinct().collect()  # Obtener todos los clústeres únicos

# Iterar sobre los clústeres y analizar los productos para cada uno
for cluster in clusters:
    cluster_id = cluster["prediction"]  # Obtener el ID del clúster

    # Filtrar los productos del clúster actual
    products_in_cluster = predictions.filter(predictions.prediction == cluster_id)

    # Seleccionar columnas de interés y agregar la columna de clúster
    products_in_cluster = products_in_cluster.select(
        "CUSTOMERNAME",
        "PRODUCTLINE",
        "SALES",
        "QUANTITYORDERED",
        "prediction"  # Agregar la columna de predicción
    )

    # Mostrar los resultados para el clúster actual
    print(f"\nProductos en el Clúster {cluster_id}:")
    products_in_cluster.show(truncate=False)

# Cierre de la sesión de Spark
#spark.stop()

"""# PRODUCTOS MAS VENDIDOS POR CLUSTER

"""

# Paso 7: Listar todos los clústeres
clusters = predictions.select("prediction").distinct().collect()  # Obtener todos los clústeres únicos

# Número de productos más vendidos que deseas listar
N = 5

# Iterar sobre los clústeres y analizar los productos para cada uno
for cluster in clusters:
    cluster_id = cluster["prediction"]  # Obtener el ID del clúster

    # Filtrar los productos del clúster actual
    products_in_cluster = predictions.filter(predictions.prediction == cluster_id)

    # Seleccionar columnas de interés
    products_in_cluster = products_in_cluster.select(
        "CUSTOMERNAME",
        "PRODUCTLINE",
        "SALES",
        "QUANTITYORDERED",
        "prediction"  # Agregar la columna de predicción
    )

    # Ordenar los productos por las ventas (SALES) de mayor a menor
    top_products_in_cluster = products_in_cluster.orderBy("SALES", ascending=False).limit(N)

    # Mostrar los resultados para el clúster actual
    print(f"\nTop {N} productos más vendidos en el Clúster {cluster_id}:")
    top_products_in_cluster.show(truncate=False)

# Cierre de la sesión de Spark
#spark.stop()

import matplotlib.pyplot as plt

# Paso 8: Extraer datos para graficar
# Convertir el DataFrame de predicciones a un Pandas DataFrame para la visualización
predictions_pd = predictions.toPandas()

# Graficar
plt.figure(figsize=(10, 6))

# Graficar los diferentes clústeres
for cluster in predictions_pd['prediction'].unique():
    cluster_data = predictions_pd[predictions_pd['prediction'] == cluster]
    plt.scatter(cluster_data['QUANTITYORDERED'], cluster_data['SALES'], label=f'Cluster {cluster}', alpha=0.6)

# Configuración del gráfico
plt.title('Visualización de Clústeres')
plt.xlabel('Cantidad Pedida')
plt.ylabel('Ventas')
plt.legend()
plt.grid()
plt.show()

# Convertir el DataFrame de predicciones a un DataFrame de pandas
predictions_pd = predictions.toPandas()

"""# **GRÁFICOS**

## Gráfico de Lineas
Análisis: Al agrupar las ventas por fecha, se examinan las tendencias a lo largo del tiempo para un clúster específico.

# Patrón de comportamiento
"""

import matplotlib.pyplot as plt

# Extraer el año y mes de la columna ORDERDATE
predictions_pd['Year'] = predictions_pd['ORDERDATE'].dt.year
predictions_pd['Month'] = predictions_pd['ORDERDATE'].dt.month

# Agrupar por clúster, año y mes, y calcular el promedio de ventas
monthly_sales = predictions_pd.groupby(['prediction', 'Year', 'Month']).agg({'SALES': 'mean'}).reset_index()

# Graficar las ventas promedio por mes y clúster
plt.figure(figsize=(15, 8))
for cluster_id in predictions_pd['prediction'].unique():
    cluster_sales = monthly_sales[monthly_sales['prediction'] == cluster_id]
    plt.plot(cluster_sales['Month'], cluster_sales['SALES'], label=f'Clúster {cluster_id}')

# Configuración del gráfico
plt.title('Promedio de Ventas por Mes y Clúster')
plt.xlabel('Mes')
plt.ylabel('Promedio de Ventas')
plt.legend(title='Clúster')
plt.grid(True)

# Mostrar el gráfico
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# Asegurarse de que 'ORDERDATE' esté en formato datetime
predictions_pd['ORDERDATE'] = pd.to_datetime(predictions_pd['ORDERDATE'])

# 1. Agrupar por clúster y producto para obtener las ventas totales de cada producto
top_products = predictions_pd.groupby(['prediction', 'PRODUCTLINE']).agg({'SALES': 'sum'}).reset_index()

# 2. Ordenar para obtener el producto más vendido por clúster
top_products = top_products.sort_values(['prediction', 'SALES'], ascending=[True, False])

# 3. Obtener el producto más vendido de cada clúster
top_products = top_products.groupby('prediction').first().reset_index()

# 4. Graficar el producto más vendido por clúster
plt.figure(figsize=(10, 6))
plt.bar(top_products['prediction'].astype(str), top_products['SALES'], color='skyblue')

# Añadir etiquetas en las barras para mostrar el nombre del producto y las ventas
for i, row in top_products.iterrows():
    plt.text(i, row['SALES'] + 50, f'{row["PRODUCTLINE"]} ({row["SALES"]:.2f})', ha='center', va='bottom', fontsize=9)

# Configuración del gráfico
plt.title('Producto Más Vendido por Clúster')
plt.xlabel('Clúster')
plt.ylabel('Ventas Totales')
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Mostrar el gráfico
plt.show()

"""# Analisis de datos: CLUSTERES

# INFORME EXCEL
"""

# -*- coding: utf-8 -*-
"""Reporte de Clústeres y Análisis de Productos Más Vendidos por Clúster"""

import pandas as pd
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
from pyspark.sql import functions as F

# Paso 1: Configuración de Spark
spark = SparkSession.builder \
    .appName("Clustering de Clientes") \
    .getOrCreate()

# Paso 2: Cargar datos
file_path = "fragance-dataset.csv"
df = spark.read.csv(file_path, header=True, inferSchema=True, sep=';')

# Preprocesamiento de datos
features = ["QUANTITYORDERED", "PRICEEACH", "SALES"]
vec_assembler = VectorAssembler(inputCols=features, outputCol="features")
df_vector = vec_assembler.transform(df)

# Normalización de características
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures", withMean=True, withStd=True)
scaler_model = scaler.fit(df_vector)
df_scaled = scaler_model.transform(df_vector)

# Aplicar K-means
kmeans = KMeans(featuresCol="scaledFeatures", k=5)  # 'k' es el número de clusteres
model = kmeans.fit(df_scaled)

# Predicción de clústeres
predictions = model.transform(df_scaled)

# Paso 6: Análisis de clústeres (Comportamiento de Compra)
clustering_analysis = predictions.groupBy("prediction").agg(
    F.avg("SALES").alias("Promedio Ventas"),
    F.avg("QUANTITYORDERED").alias("Promedio Cantidad Pedida"),
    F.count("ORDERNUMBER").alias("Número de Órdenes")
)

# Ordenar los resultados por 'Promedio Ventas' de mayor a menor
clustering_analysis_sorted = clustering_analysis.orderBy("Promedio Ventas", ascending=False)

# Convertir clustering_analysis_sorted a pandas para exportar
clustering_analysis_pd = clustering_analysis_sorted.toPandas()

# Lista de todos los clústeres
clusters = predictions.select("prediction").distinct().collect()

# Crear una lista para todos los productos de todos los clústeres
all_products_data = []

# Iterar sobre los clústeres y analizar los productos para cada uno
for cluster in clusters:
    cluster_id = cluster["prediction"]

    # Filtrar los productos del clúster actual
    products_in_cluster = predictions.filter(predictions.prediction == cluster_id)

    # Seleccionar columnas de interés
    products_in_cluster = products_in_cluster.select(
        "CUSTOMERNAME",
        "PRODUCTLINE",
        "SALES",
        "QUANTITYORDERED",
        "prediction",   # Agregar la columna de predicción
        "ORDERDATE"
    )


    # Convertir el DataFrame de Spark a Pandas
    products_in_cluster_pd = products_in_cluster.toPandas()

    # Agregar una columna para identificar el clúster
    products_in_cluster_pd['Cluster_ID'] = cluster_id

    # Agregar los datos del clúster al conjunto global
    all_products_data.append(products_in_cluster_pd)

# Unir todos los DataFrames de productos de todos los clústeres en uno solo
all_products_df = pd.concat(all_products_data, ignore_index=True)

# Crear un archivo Excel con múltiples hojas: una para análisis de clústeres y otra para productos más vendidos
with pd.ExcelWriter("Cluster_Analysis_Report.xlsx") as writer:
    # Exportar análisis de clústeres
    clustering_analysis_pd.to_excel(writer, sheet_name="Análisis de Clústeres", index=False)

    # Exportar todos los productos por clúster en una tabla única
    all_products_df.to_excel(writer, sheet_name="Productos por Clúster", index=False)

# Confirmar que el reporte se ha generado
print("El reporte de análisis de clústeres y productos más vendidos se ha generado exitosamente en 'Cluster_Analysis_Report.xlsx'.")

# Cierre de la sesión de Spark
spark.stop()

import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# Paso 1: Configuración de Spark
spark = SparkSession.builder \
    .appName("Clustering de Productos") \
    .getOrCreate()

# Paso 2: Cargar los datos
file_path = "fragance-dataset.csv"
df = spark.read.csv(file_path, header=True, inferSchema=True, sep=';')

# Paso 3: Preprocesamiento
features = ["QUANTITYORDERED", "PRICEEACH", "SALES"]
vec_assembler = VectorAssembler(inputCols=features, outputCol="features")
df_vector = vec_assembler.transform(df)

scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures", withMean=True, withStd=True)
scaler_model = scaler.fit(df_vector)
df_scaled = scaler_model.transform(df_vector)

# Paso 4: Aplicar K-means
kmeans = KMeans(featuresCol="scaledFeatures", k=5)  # 'k' es el número de clusteres
model = kmeans.fit(df_scaled)

# Predicción de clústeres
predictions = model.transform(df_scaled)

# Paso 5: Crear una tabla completa con todos los productos por clúster
# No limitamos los productos por clúster, agregamos todos los datos

# Seleccionar las columnas de interés y agregar la columna de predicción
all_products = predictions.select(
    "CUSTOMERNAME",
    "PRODUCTLINE",
    "SALES",
    "QUANTITYORDERED",
    "ORDERDATE",
    "prediction"  # Columna con el clúster asignado
)

# Convertir el DataFrame de PySpark a Pandas para exportar a Excel
all_products_pd = all_products.toPandas()

# Paso 6: Guardar el DataFrame completo en un archivo Excel
# Guardamos todos los datos de los productos por clúster en diferentes hojas de Excel
excel_filename = "productos_por_cluster.xlsx"

# Escribir los datos en diferentes hojas, una por cada clúster
with pd.ExcelWriter(excel_filename) as writer:
    clusters = sorted(all_products_pd['prediction'].unique())  # Ordenamos los clústeres

    for cluster_id in clusters:
        # Filtrar los productos para el clúster actual
        cluster_data = all_products_pd[all_products_pd['prediction'] == cluster_id]

        # Guardar los datos de ese clúster en una hoja del Excel
        cluster_data.to_excel(writer, sheet_name=f'Cluster_{cluster_id}', index=False)

print(f"Los datos se han exportado correctamente a {excel_filename}.")

